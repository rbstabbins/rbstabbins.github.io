---
layout: page
title: About
permalink: /about/
order: 1
---

<img style="float: left; padding: 0px 30px 0px 0px; max-width: 45%; height:auto"        
        height=460
        width=285
        src="lift_selfie_crop.jpg"
        title="Self portrait in a lift. May 2023." >

I'm interested in how we use robotic avatars to visually explore remote places in the Solar System.  

<!-- For each of the planets and moons whose surfaces we visit, I'd like to know what kind of visual system an organism might have evolved, if its survival depended on answering the same questions that we ask when visiting these places. -->

Some of the things I spend time thinking about include how light interacts with the different materials that make up the Solar System, how it travels towards, across and away from a landscape of those materials, how it encodes these interactions in a pattern across our camera sensors, how we represent that pattern in a digital signal, and how we process these signals into rich descriptions of those remote places.

Right now I'm a JSPS Short-Term Fellow in Tokyo, Japan, where I've joined the science team of the JAXA Martian Moons Exploration mission (MMX), to help with calibration and operations planning for the OROCHI multispectral imager. MMX will launch in 2024 and reach the Mars system at the end of the summer 2025, studying Phobos from a Quasi-Satellite Orbit before landing on the surface and collecting a sample, and returning this to Earth in 2029, via a phase of Deimos observations.

Previously I was a Post-Doctoral Research Associate with the [Planetary Surfaces Group](https://www.planetsurf.space/){:target="_blank"} of the Natural History Museum (London, UK), and before that I was a PhD student in the [Planetary Sciences Group](https://www.ucl.ac.uk/mssl/research/solar-system/planetary-science){:target="_blank"} at the [Mullard Space Science Laboratory](https://www.ucl.ac.uk/mssl/mullard-space-science-laboratory){:target="_blank"}, University College London. There are some [other](other.markdown) things I'm interested in too.

<figure>
        <img style="display: block; padding: 20px 20px 5px 20px; max-width:80%; margin-left:auto; margin-right:auto"
                width="80%"
                src="profile_rikkyo.jpg"
                alt="Roger is standing in the street that leads to the Rikkyo University Guest house, whearing sunglasses, a backpack and cylinder bag, and the sun is very bright, so he is squinting and smiling." >
        <figcaption style="text-align: center; font-style: italic">A sunny day outside the guest house at Rikkyo University. 4/2023</figcaption>
</figure>

# Biography
<!-- I was born in London but grew up in Cornwall, in the Helford area, next to the sea and under the stars.
I did a lot of sailing growing up, through the Optimist and Laser National junior and youth programmes, but then became more interested in music, in the orbit of the Falmouth scene circa-2010. -->
I studied Physics in London at UCL from 2010 - 2014, developing an interest in robotics through an internship with the Autonomous Systems Group at RAL Space, and an interest in space instrumentation and imaging during my Masters thesis.
&nbsp;

In 2014 I joined the [Media & Arts Technology](https://mat.qmul.ac.uk/){:target="_blank"} Centre for Doctoral Training in EECS at QMUL to pursue my intersts in digital musical instruments. I learned a lot about programming (C++, MaxMSP, Arduino) and human-computer interaction, but mostly I learned that what I really wanted to explore was outer space, and that my musical explorations of inner space were best done outside of the academy.
I sat the Masters year, completing modules in Interaction Design, C++ for Image Processing and various mandatory courses in audio, interactive media and research methods, and I was lucky enough to spend a summer on placement at the Soho offices of M&C Saatchi, where Chief Innovation Officer Dave Cox let me play with Unity and hobby drone kits in an attempt to build a haptic torque controller for virtual reality.
&nbsp;

After leaving QMUL, in 2015 I was offered a place in the [Planetary Sciences Group](https://www.ucl.ac.uk/mssl/research/solar-system/planetary-science){:target="_blank"} at University College London's [Mullard Space Science Laboratory](https://www.ucl.ac.uk/mssl/mullard-space-science-laboratory){:target="_blank"}, to join the team developing the panoramic multispectral-stereo camera system, [PanCam](https://exploration.esa.int/web/mars/-/45103-rover-instruments?section=pancam---the-panoramic-camera){:target="_blank"}, for ESA's [ExoMars 2022 Rover](https://exploration.esa.int/web/mars/-/48088-mission-overview){:target="_blank"}, as a UK Space Agency funded PhD student, under the supervision of Andrew Coates.
&nbsp;

For my thesis I focused on modelling the response function of PanCam, producing a mathematical model of the system optics and detector, and implementing this in a software simulator. I used this simulator to help develop the auto-exposure algorithm of the camera, and to investigate the effects of noise and nonlinearity on the calibration process. I also used this to inform my contributions to the ExoSpec toolkit for spectral image analysis.
&nbsp;

In 2019 I joined Peter Grindrod's [Planetary Surfaces Group](https://www.planetsurf.space/){:target="_blank"} at the Natural History Museum in London, to continue preparations for spectral imaging with PanCam as a UK Space Agency Aurora PDRA. 

For this project I investigated how we can use prior knowledge of the landing site to guide our selection of combinations of spectral filters for efficiently characterising the geology that the ExoMars rover will explore. I developed a method for exhaustively but efficiently searching through all the spectral parameters available to PanCam, and paired combinations thereof, and used a simple parallelised implementation of Linear Discriminant Analysis to evaluate and rank these. The result is a quick method for finding the smallest number of filters required to maximise the contrast between a defined material and an expected background. Details of the publication will be available shortly (4/6/2023).
&nbsp;
