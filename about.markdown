---
layout: page
title: About
permalink: /about/
order: 1
---

I'm interested in how we use robotic avatars to visually explore remote places in the Solar System.  

For each of the planets and moons whose surfaces we visit, I'd like to know what kind of visual system an organism might have evolved, if its survival depended on answering the same questions that we ask when visiting these places.

Some of the things I spend time thinking about include how light interacts with the different materials that make up the Solar System, how it travels towards, across and away from a landscape of those materials, how it encodes these interactions in a pattern across our camera sensors, how we represent that pattern in a digital signal, and how we process these signals into rich descriptions of those remote places.

<img style="float: left; padding: 0px 0px 30px 0px;"
        width="100%"
        src="profile_rikkyo.jpg"
        alt="A sunny day outside the guest house at Rikkyo University. April 2023." >

Right now I'm a JSPS Short-Term Fellow in Tokyo, Japan, where I've joined the science team of the JAXA Martian Moons Exploration mission (MMX), to help with calibration and operations planning for the OROCHI multispectral imager. MMX will launch in 2024 and reach the Mars system at the end of the summer 2025, first studying Phobos, then landing on the surface and collecting a sample, and returning this to Earth in 2029, via a phase of Deimos observations.

Previously I was a Post-Doctoral Research Associate with the [Planetary Surfaces Group](https://www.planetsurf.space/){:target="_blank"} of the Natural History Museum (London, UK), and before that I was a PhD student in the [Planetary Sciences Group](https://www.ucl.ac.uk/mssl/research/solar-system/planetary-science){:target="_blank"} at the [Mullard Space Science Laboratory](https://www.ucl.ac.uk/mssl/mullard-space-science-laboratory){:target="_blank"}, University College London.

There are some [other](other.markdown) things I'm interested in too.

# Biography
<!-- I was born in London but grew up in Cornwall, in the Helford area, next to the sea and under the stars.
I did a lot of sailing growing up, through the Optimist and Laser National junior and youth programmes, but then became more interested in music, in the orbit of the Falmouth scene circa-2010. -->
I studied Physics in London at UCL from 2010 - 2014, developing an interest in robotics through an internship with the Autonomous Systems Group at RAL Space, and an interest in space instrumentation and imaging during my Masters thesis.
&nbsp;

In 2014 I joined the Media & Arts Technology Centre for Doctoral Training in EECS at QMUL to pursue my intersts in digital musical instruments. I learned a lot about programming (C++, MaxMSP, Arduino) and human-computer interaction, but most importantly I learned that I needed to be working in the space domain.
<!-- I sat the Masters year, completing modules in Interaction Design, C++ for Image Processing and various mandatory courses in audio, interactive media and research methods, and I was lucky enough to spend a summer on placement in the heart of SoHo in the offices of M&C Saatchi, where Dave Cox let me play with Unity and hobby drone kits in an attempt to build a haptic torque controller for virtual reality. -->
&nbsp;

After leaving QMUL, in 2015 I was offered a place in the Planetary Science Group at University College London's [Mullard Space Science Laboratory](https://www.ucl.ac.uk/mssl/mullard-space-science-laboratory), to join the team preparing for the operation of the panoramic multispectral-stereo camera system, [PanCam](https://exploration.esa.int/web/mars/-/45103-rover-instruments?section=pancam---the-panoramic-camera), for ESA's [ExoMars 2022 Rover](https://exploration.esa.int/web/mars/-/48088-mission-overview), under the supervision of Andrew Coates.
&nbsp;

For my thesis I focused on modelling the response function of PanCam, producing a mathematical model of the system optics and detector, and implementing this in a software simulator. I used this simulator to help develop the auto-exposure algorithm of the camera, and to investigate the effects of noise and nonlinearity on the calibration process. I also used this to inform my contributions to the ExoSpec toolkit for spectral image analysis.
&nbsp;

In 2019 I joined Peter Grindrod's Planetary Surfaces Group at the Natural History Museum in London, to continue preparations for spectral imaging with PanCam as a UK Space Agency Aurora PDRA. 

For this project I investigated how we can use prior knowledge of the landing site to guide our selection of combinations of spectral filters for efficiently characterising the geology that the ExoMars rover will explore. I developed a method for exhaustively but efficiently searching through all the spectral parameters available to PanCam, and paired combinations thereof, and used a simple parallelised implementation of Linear Discriminant Analysis to evaluate and rank these. The result is a quick method for finding the smallest number of filters required to maximise the contrast between a defined material and an expected background. Details of the publication will be available shortly (4/6/2023).
&nbsp;
